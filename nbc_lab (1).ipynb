{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lti3WORYZsmK-webhook"
      },
      "source": [
        "## Our task is to classify spam/ham messages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRv9oWNGQHor"
      },
      "source": [
        "## 0 - Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1GrD0GIFQHos",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "0e01a0fc-100d-46cd-ff2d-4d46abd80a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=98acc95db74de281f3e7f1dd0536bac5ab95a72d03c0b0c4f85768156c0b1c2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'spam.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install wget\n",
        "import wget\n",
        "wget.download('https://dru.fra1.digitaloceanspaces.com/DS_Fundamentals/datasets/04_supervised_learning/Naive_Bayes_Classifier/spam.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F86OV8dwZsmL"
      },
      "source": [
        "## 1 - Packages ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eEoj1DOwZsmM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5Wy6bYpZsmO"
      },
      "source": [
        "## 2 - Overview of the Problem set ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DtUaewx1ZsmP"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "    df_for_tests = df.head()\n",
        "\n",
        "    idx = np.arange(df.shape[0])\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    train_set_size = int(df.shape[0] * 0.8)\n",
        "\n",
        "    train_set = df.loc[idx[:train_set_size]]\n",
        "    test_set = df.loc[idx[train_set_size:]]\n",
        "\n",
        "    return train_set, test_set, df_for_tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ae4YTbLtZsmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8efc8bf-79a2-4837-fa2b-cab558f3d217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        v1                                                 v2\n",
            "3600   ham                       Jay told me already, will do\n",
            "3280   ham  I tot it's my group mate... Lucky i havent rep...\n",
            "1936   ham      My planning usually stops at \\find hella weed\n",
            "157    ham  Hello, my love. What are you doing? Did you ge...\n",
            "3439   ham                     What time you thinkin of goin?\n",
            "...    ...                                                ...\n",
            "3314  spam  FREE MESSAGE Activate your 500 FREE Text Messa...\n",
            "1493   ham  How are you with moneY...as in to you...money ...\n",
            "1516   ham  I need to come home and give you some good lov...\n",
            "3664   ham              Ha... U jus ate honey ar? So sweet...\n",
            "2911   ham  You didn't have to tell me that...now i'm thin...\n",
            "\n",
            "[4457 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "train_set, test_set, df_for_tests = load_data()\n",
        "print(train_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fxIXA7FZsmU"
      },
      "source": [
        "## 3 - Naive Bayes Classifier\n",
        "**Mathematical expression of the algorithm**:\n",
        "\n",
        "\n",
        "This algorithm is based on Bayes' theorem:\n",
        "    \\begin{equation}\n",
        "    P(A_{j}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}x_{1},\\dots,x_{n}) = \\frac{P(x_{1},\\dots,x_{n}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})P(A_{j})}{P(x_{1},\\dots,x_{n})}\n",
        "    \\end{equation}\n",
        "    \n",
        "Ignoring denominator (because it stays the same for all cases):\n",
        "\n",
        "$$ \\begin{equation}\n",
        "    P(A_{j})P(x_{1},\\dots,x_{n}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j}) = P(A_{j}, x_{1},\\dots,x_{n}) = \\\\\n",
        "  \\hspace{1cm} = P(x_{1}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}x_{2},\\dots,x_{n}, A_{j})P(x_{2}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}x_{3}, \\dots ,x_{n}, A_{j})\\dots P(x_{n-1}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}x_{n}, A_{j}) \\approx \\\\\n",
        "  \\hspace{1cm}\n",
        "  \\end{equation}$$\n",
        "By making an assumption that the $x_{i}$ are conditionally independent of each other:\n",
        "$$ \\begin{equation} \\approx P(A_{j}) \\prod_{i=1}^{n} P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})\n",
        "   \\end{equation}$$\n",
        "   \n",
        "We can calculate the probability, if we know the prior probability:\n",
        "\n",
        "$$ \\begin{equation}\n",
        "    y^{*} = \\operatorname*{arg\\,max}_{j} \\big(P(A_{j}) \\prod_{i=1}^{n} P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})\\big)\n",
        "   \\end{equation}$$\n",
        "   \n",
        "   \n",
        "Due to floating point underflow, the above is usually replaced with the numerically tractable expression:\n",
        "\n",
        "$$ \\begin{equation}\n",
        "    y^{*} = \\operatorname*{arg\\,max}_{j} \\big( \\ln(P(A_{j})) + \\sum_{i=1}^{n} \\ln(P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})) \\big)\n",
        "   \\end{equation}$$\n",
        "   \n",
        "For more consistent knowledge of the NBC algorithm (https://web.stanford.edu/~jurafsky/slp3/4.pdf)\n",
        "\n",
        "**Training the Naive Bayes Classifier**:\n",
        "\n",
        "How can we find the probabilities $\\ln(P(A_{j}))$ and $\\ln(P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j}))$ ? We'll simply use the frequencies in the data. For the class prior $P(A_{j})$ we find what percentage of the messages in our training set are in each class $A_{j}$:\n",
        "\n",
        "$$ \\begin{equation}\n",
        "    \\ln(P(A_{j})) = \\ln\\big(\\frac{N_{A_{j}}}{N}\\big)\n",
        "    \\tag{1}\n",
        "   \\end{equation}$$\n",
        "\n",
        "where $N_{A_{j}}$ is the number of messages in our training data with class $A_{j}$ and $N$ be the total number of messages.\n",
        "\n",
        "In $P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})$ we just compute as the fraction of times the word $x_{i}$ appears among all words in all messages of class $A_{j}$:\n",
        "\n",
        "$$ \\begin{equation}\n",
        "    \\ln(P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})) = \\ln\\big(\\frac{ N_{x_i, A_j}}{|A_{j}|} \\big)\n",
        "    \\tag{2}\n",
        "   \\end{equation}\n",
        "$$\n",
        "\n",
        "where $N_{x_i, A_j}$ is number of times word $x_{i}$ appears in messages from class $A_{j}$ and $|A_{j}|$ - total count of all words in class $A_{j}$.\n",
        "\n",
        "   \n",
        "#### Laplace smoothing\n",
        "\n",
        "In statistics, additive smoothing, also called Laplace smoothing, or Lidstone smoothing, is a technique that is used to smooth categorical data. Given an observation\n",
        "$\\begin{equation}\n",
        "    message = (x_{1}\\, \\dots \\,x_{n})\n",
        " \\end{equation}$, a \"smoothed\" version of the data gives the estimator:\n",
        "\n",
        "\n",
        "$$ \\begin{equation}\n",
        "    \\ln(P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})) = \\ln\\big(\\frac{ N_{x_i, A_j} + \\alpha}{ |A_{j}| +  \\alpha * |V|} \\big)\n",
        "    \\tag{3}\n",
        "   \\end{equation}\n",
        "$$\n",
        "\n",
        "where the pseudocount\n",
        "$\\begin{equation}\n",
        "    \\alpha > 0\n",
        " \\end{equation}$ is the smoothing parameter (\n",
        "$\\begin{equation}\n",
        "    \\alpha = 0\n",
        " \\end{equation}$ corresponds to no smoothing) and $V$ is a vocabulary, which consists of the union of all the words in all classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9mF4dTVZsmU"
      },
      "source": [
        "### 3.1 - Preprocessing the data\n",
        "\n",
        "Let's clean our data and leave only letters a-z, A-Z, numbers 0-9 and cast all letters to lowercase, replace double to n spaces with just one space, remove trailing spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OZDyrOMZZsmV"
      },
      "outputs": [],
      "source": [
        "# Clean the data\n",
        "\n",
        "def clean_data(message):\n",
        "\n",
        "    message = message.lower()\n",
        "    cleaned_message = ''.join(char if char.isalnum() or char.isspace() else ' ' for char in message)\n",
        "    cleaned_message = ' '.join(cleaned_message.split())\n",
        "    return cleaned_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r96K1BnlZsmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b32139-8009-4c75-834a-eafa537c743a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cleaned:  doesn t get how to operate 66 7 after it lt gt won t or what\n",
            "cleaned:  o k lar7i double check wif da hair dresser already he said 77 88 5 wun cut v short question std txt rate t c s\n"
          ]
        }
      ],
      "source": [
        "sentence_1 = 'Doesn\\'t get, how{to}% \\\\operate+66.7 :after[it]\"\" & lt;# & gt; won\\'t `or(what)'\n",
        "sentence_2 = 'O\\]k,.lar7i$double{} check wif*& da! hair: [dresser;   ..already He SaID-77.88.5 wun cut v short question(std txt rate)T&C\\'s'\n",
        "print('cleaned: ',clean_data(sentence_1))\n",
        "print('cleaned: ',clean_data(sentence_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gEFD2wuFZsma"
      },
      "outputs": [],
      "source": [
        "# Preparation data for model\n",
        "\n",
        "def prep_for_model(train_set, test_set):\n",
        "\n",
        "\n",
        "    train_set = train_set.copy()\n",
        "    test_set = test_set.copy()\n",
        "\n",
        "    train_set['cleaned_message'] = (train_set['v2']).apply(clean_data)\n",
        "    test_set['cleaned_message'] = (test_set['v2']).apply(clean_data)\n",
        "\n",
        "#     print(test_set)\n",
        "\n",
        "    train_set_x = np.array(train_set['cleaned_message'].apply(str.split))\n",
        "#     train_set_x = [item for sublist in train_set_x for item in sublist]\n",
        "\n",
        "#     print(train_set_x)\n",
        "\n",
        "    test_set_x = np.array(test_set['cleaned_message'].apply(str.split))\n",
        "#     test_set_x = [item for sublist in test_set_x for item in sublist]\n",
        "\n",
        "    train_set_y = np.array(train_set['v1'])\n",
        "    test_set_y = np.array(test_set['v1'])\n",
        "\n",
        "    return train_set_x, train_set_y, test_set_x, test_set_y\n",
        "\n",
        "train_set_x, train_set_y, test_set_x, test_set_y = prep_for_model(train_set, test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bnNLInEBZsmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "955bf822-4a89-4b7e-dc20-ce5e91a50dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ham ['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']\n",
            "ham ['u', 'dun', 'say', 'so', 'early', 'hor', 'u', 'c', 'already', 'then', 'say']\n"
          ]
        }
      ],
      "source": [
        "a1, a2, b1, b2 = prep_for_model(df_for_tests.head(3), df_for_tests.tail(2))\n",
        "print(a2[0], a1[0])\n",
        "print(b2[0], b1[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn_Ag9ZQZsmg"
      },
      "source": [
        "Now let's check words in each category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "olXobIliZsmg"
      },
      "outputs": [],
      "source": [
        "# Check words in categories\n",
        "\n",
        "def categories_words(x_train, y_train):\n",
        "\n",
        "    all_words_list = []\n",
        "    ham_words_list = []\n",
        "    spam_words_list = []\n",
        "\n",
        "    for i in range(len(x_train)):\n",
        "        word = x_train[i]\n",
        "        label = y_train[i]\n",
        "\n",
        "        all_words_list.extend(word)\n",
        "\n",
        "        if label == 'ham':\n",
        "            ham_words_list.extend(word)\n",
        "        elif label == 'spam':\n",
        "            spam_words_list.extend(word)\n",
        "\n",
        "    all_words_list = np.array(all_words_list)\n",
        "    ham_words_list = np.array(ham_words_list)\n",
        "    spam_words_list = np.array(spam_words_list)\n",
        "\n",
        "\n",
        "    return all_words_list, ham_words_list, spam_words_list\n",
        "\n",
        "all_words_list_a1, ham_words_list_a1, spam_words_list_a1 = categories_words(a1, a2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zCkJB-7TZsmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e564c7fb-7715-4e95-8bed-f291b4faf949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first five \"ham\" words of a1:  ['go' 'until' 'jurong' 'point' 'crazy']\n"
          ]
        }
      ],
      "source": [
        "print('first five \"ham\" words of a1: ', ham_words_list_a1[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0Izv7t7gCAL"
      },
      "source": [
        "Let's calculate probability of each word in a category, use\n",
        "\n",
        "---\n",
        "\n",
        "(3) formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eo_VKmc9QLsF"
      },
      "outputs": [],
      "source": [
        "# Calculate log probability of each word in a category using smoothing\n",
        "def calc_words_probs_of_category(category_words_list, all_words_list, alpha):\n",
        "\n",
        "    category_words_dict = {}\n",
        "\n",
        "    word_counts = {}\n",
        "    num_category_words = 0\n",
        "\n",
        "    # Count occurrences of words in the specific category\n",
        "    for word in category_words_list:\n",
        "        word_counts[word] = word_counts.get(word, 0) + 1\n",
        "        num_category_words += 1\n",
        "\n",
        "    num_unique_all_words = len(set(all_words_list))\n",
        "\n",
        "    # Calculate log probabilities for each word\n",
        "    for word in all_words_list:\n",
        "        w_count = word_counts.get(word, 0)\n",
        "        prob = np.log((w_count + alpha) / (num_category_words + alpha * num_unique_all_words))\n",
        "        category_words_dict[word] = prob\n",
        "    # word_counts = Counter(category_words_list)\n",
        "    # num_unique_category_words = len(set(category_words_list))\n",
        "    # num_unique_all_words = len(set(all_words_list))\n",
        "\n",
        "    # for w in all_words_list:\n",
        "    #     w_count = word_counts[w]\n",
        "    #     prob = np.log((w_count + alpha)/(num_unique_category_words + alpha*num_unique_all_words))\n",
        "    #     category_words_dict[w] = prob\n",
        "\n",
        "    return category_words_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Y1eccDrUQL1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f23a70b-d281-4c0d-9686-ea4110c5103e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The log prob of the word “87121” from the \"ham\" category in a1: -4.3694478524670215\n",
            "\n",
            "The log prob of the word “87121” from the \"spam\" category in a1: -3.7612001156935624\n"
          ]
        }
      ],
      "source": [
        "ham_words_dict_a1 = calc_words_probs_of_category(ham_words_list_a1, all_words_list_a1, alpha=1)\n",
        "print('The log prob of the word “87121” from the \"ham\" category in a1:', ham_words_dict_a1['87121'])\n",
        "\n",
        "spam_words_dict_a1 = calc_words_probs_of_category(spam_words_list_a1, all_words_list_a1, alpha=1)\n",
        "print('\\nThe log prob of the word “87121” from the \"spam\" category in a1:', spam_words_dict_a1['87121'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P5kBQrTgi2g"
      },
      "source": [
        "Сalculating the prior probability for each class category, use (1) formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Bz2-UTgEQL-B"
      },
      "outputs": [],
      "source": [
        "# Calculate prior log probability of each category\n",
        "def calc_prior_category_prob(y_train):\n",
        "\n",
        "    unique_categories, category_counts = np.unique(y_train, return_counts=True)\n",
        "\n",
        "    ham_index = np.where(unique_categories == 'ham')[0][0]\n",
        "    spam_index = np.where(unique_categories == 'spam')[0][0]\n",
        "\n",
        "    count_ham = category_counts[ham_index] if 'ham' in unique_categories else 0\n",
        "    count_spam = category_counts[spam_index] if 'spam' in unique_categories else 0\n",
        "\n",
        "    prior_ham_prob = np.log(count_ham / len(y_train)) if len(y_train) > 0 else 0\n",
        "    prior_spam_prob = np.log(count_spam / len(y_train)) if len(y_train) > 0 else 0\n",
        "    # category_counts = Counter(y_train)\n",
        "    # count_ham = category_counts['ham']\n",
        "    # count_spam = category_counts['spam']\n",
        "    # prior_ham_prob = np.log(count_ham/len(y_train))\n",
        "    # prior_spam_prob = np.log(count_spam/len(y_train))\n",
        "    # return prior_ham_prob, prior_spam_prob\n",
        "\n",
        "    return prior_ham_prob, prior_spam_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uC9AAWPRQcsE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71c92b7-e3f6-4dc3-d9c6-f6612954d7cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prior log probability of \"ham\" category in a2:  -0.40546510810816444\n",
            "Prior log probability of \"spam\" category in a2:  -1.0986122886681098\n"
          ]
        }
      ],
      "source": [
        "print('Prior log probability of \"ham\" category in a2: ', calc_prior_category_prob(a2)[0])\n",
        "print('Prior log probability of \"spam\" category in a2: ', calc_prior_category_prob(a2)[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AARnBVXGZsmk"
      },
      "source": [
        "### 3.2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LO7_8FEKZsmm"
      },
      "outputs": [],
      "source": [
        "class Naive_Bayes(object):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    -----------\n",
        "    alpha: int\n",
        "        The smoothing coeficient.\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha):\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.train_set_x = None\n",
        "        self.train_set_y = None\n",
        "\n",
        "        self.all_words_list = []\n",
        "        self.ham_words_list = []\n",
        "        self.spam_words_list = []\n",
        "\n",
        "        self.ham_words_dict = {}\n",
        "        self.spam_words_dict = {}\n",
        "\n",
        "        self.prior_ham_prob = None\n",
        "        self.prior_spam_prob = None\n",
        "\n",
        "\n",
        "    def fit(self, train_set_x, train_set_y):\n",
        "\n",
        "        # Generate all_words_list, ham_words_list, spam_words_list using function 'categories_words';\n",
        "        # Calculate probability of each word in both categories using function 'calc_words_probs_of_category';\n",
        "        # Calculate prior probability of each category using function 'calc_prior_category_prob'.\n",
        "        all_words_list, ham_words_list, spam_words_list = categories_words(train_set_x, train_set_y)\n",
        "        self.ham_probs = calc_words_probs_of_category(ham_words_list, all_words_list, self.alpha)\n",
        "        self.spam_probs = calc_words_probs_of_category(spam_words_list, all_words_list, self.alpha)\n",
        "        self.prior_ham_prob, self.prior_spam_prob = calc_prior_category_prob(train_set_y)\n",
        "\n",
        "\n",
        "    def predict(self, test_set_x):\n",
        "\n",
        "        # Calculate probabilities of belonging to ham and spam category\n",
        "        # Compare these probabilities and choose the max\n",
        "        # Return list of predicted labels for test set; type(prediction) -> list, len(prediction) = len(test_set_y)\n",
        "        prediction = []\n",
        "        for message in test_set_x:\n",
        "            ham_prob = self.prior_ham_prob + sum(self.ham_probs[word] for word in message if word in self.ham_probs)\n",
        "            spam_prob = self.prior_spam_prob + sum(self.spam_probs[word] for word in message if word in self.spam_probs)\n",
        "            label = 'ham' if ham_prob > spam_prob else 'spam'\n",
        "            prediction.append(label)\n",
        "\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVU0yzP1Zsmo"
      },
      "source": [
        "## 4 - Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIiilw0KZsmo"
      },
      "source": [
        "First of all, we should define a smoothing coeficient (`alpha`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "two70VtTZsmp"
      },
      "outputs": [],
      "source": [
        "a = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0FVH2nMZsmr"
      },
      "source": [
        "Now we can initialize our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lFOgwc0eZsmr"
      },
      "outputs": [],
      "source": [
        "model = Naive_Bayes(alpha=a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBs3q36OZsmu"
      },
      "source": [
        "Let's train our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dh3_xodbZsmv"
      },
      "outputs": [],
      "source": [
        "model.fit(train_set_x, train_set_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MruvACv2Zsmw"
      },
      "source": [
        "## 5 - Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0-zUCJU_Zsmy"
      },
      "outputs": [],
      "source": [
        "y_predictions = model.predict(test_set_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNA1wZRAZsmy"
      },
      "source": [
        "Let's calculate accuracy (accuracy of model must be >0.95):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KBFDdUdkZsm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61446bd4-8319-43ba-bcab-fcc6cfcaf7fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9856502242152466\n"
          ]
        }
      ],
      "source": [
        "actual = list(test_set_y)\n",
        "accuracy = (y_predictions == test_set_y).mean()\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Wk8eTRZsm1"
      },
      "source": [
        "## 6 - Conclusion\n",
        "As we can see, our model fits well the hypothesis function to the data.\n",
        "\n",
        "#### What's next:\n",
        "1. Try experimenting with the `alpha` to see how this affects the model you have built.\n",
        "2. Compare the results you have obtained with the `sklearn.naive_bayes.MultinomialNB` model.\n",
        "3. Try this model in the wild! Select your favorite dataset [here](https://www.kaggle.com/datasets?sortBy=hottest&group=public&page=1&pageSize=20&size=small&filetype=all&license=all&tagids=13303) and play with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaLpZWBiQHo-"
      },
      "source": [
        "##### Naive Bayes Classifier Done!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}